### üìì Carnet de Bord : D√©veloppement U-Net Deblurring

#### 1. Probl√©matique Initiale & Contraintes
* **Architecture de base :** U-Net standard (2015).
* **Probl√®me :** Mod√®le de ~28M √† 30M de param√®tres trop lourd pour les ressources disponibles (Google Colab Free / MacBook MPS).
    * *Sympt√¥mes :* Timeout apr√®s quelques epochs, OOM (Out of Memory), it√©rations trop lentes.
* **Objectif :** Cr√©er une architecture "Lightweight" (< 5M params) capable d'apprendre efficacement sans sacrifier la capacit√© de reconstruction.

#### 2. Optimisation de l'Architecture (V1)
* **R√©duction des param√®tres :** Passage de 28M √† ~3M.
    * Remplacement des Convolutions Standards par des **Depthwise Separable Convolutions (DSConv)** (Gain : facteur ~8 sur les poids).
    * R√©duction du nombre de filtres initiaux (`start_filters`) de 64 √† 32.
    * Remplacement des `ConvTranspose2d` (lourdes, artefacts damier) par `Upsample Bilinear` + `Conv`.
* **Choix structurels :**
    * Maintien d'une convolution standard 4x4 (stride 2) pour le *Downsampling* afin de pr√©server l'information spatiale critique (responsable d'1M de params √† elle seule, mais jug√©e n√©cessaire).
    * **Global Residual Connection :** Adoption de la strat√©gie $Output = Input + Network(Input)$ pour forcer le r√©seau √† apprendre uniquement le r√©sidu (le flou) plut√¥t que de reconstruire l'image enti√®re.

#### 3. Strat√©gie de Donn√©es (Data Pipeline)
* **Training vs Inference :**
    * Entra√Ænement sur des **Random Crops (256x256)** pour g√©rer la VRAM et augmenter la diversit√© locale.
    * Validation/Test sur image compl√®te via strat√©gie de **Tiling (Tuilage)** avec *Overlap* et *Blending* pour √©viter les effets de bord sur les images HD (1280x720).
* **Data Augmentation (Crucial pour le d√©floutage) :**
    * Flip Horizontal & Vertical.
    * **Rotation 90¬∞/180¬∞/270¬∞ :** Indispensable pour varier la direction des vecteurs de flou de mouvement (transformer un flou gauche-droite en haut-bas).
    * *Note :* Application stricte de la "Joint Transform" (m√™me seed al√©atoire pour l'image floue et l'image nette).

#### 4. Adaptation Mat√©rielle (MacBook MPS)
* **Optimisation I/O :** Passage de `num_workers=4` √† `num_workers=0` pour √©viter les instabilit√©s du multiprocessing sur puce Apple Silicon.
* **Gestion M√©moire (VRAM 16Go) :**
    * D√©tection d'un OOM avec Batch Size 32 + Mod√®le 3M.
    * Ajustement du **Batch Size √† 16** (ou 32 avec *Gradient Accumulation* simul√©).
    * Utilisation de `torch.mps.empty_cache()` et `clip_grad_norm_` pour la stabilit√©.

#### 5. Analyse du Run #1 (Night Run)
* **R√©sultats :**
    * Training PSNR : **29.32 dB**
    * Validation PSNR : **27.50 dB**
* **Observations :**
    * Le mod√®le apprend bien (pas de divergence).
    * L'√©cart Train/Val (1.8 dB) sugg√®re un l√©ger "Generalization Gap", potentiellement d√ª au fait que les crops d'entra√Ænement sont parfois "faciles" (ciel uni) vs les crops de validation centr√©s (objets complexes).
    * **Probl√®me Majeur :** Le Learning Rate est rest√© constant (`2e-4`). Le scheduler `ReduceLROnPlateau` √©tait trop timide (patience trop √©lev√©e ou seuil non atteint), emp√™chant le mod√®le d'affiner les micro-d√©tails (convergence fine).

#### 6. Plan d'Action pour le Run #2 (V2)
Pour viser > 28.5 dB en validation :

1.  **Architecture (Scale Up) :** Augmentation de la largeur du mod√®le. Passage de `start_filters=32` √† **48** (environ 6.5M params) pour augmenter la capacit√© de m√©morisation des textures complexes.
2.  **Scheduler :** Remplacement de `ReduceLROnPlateau` par **`CosineAnnealingLR`**.
    * *But :* Forcer math√©matiquement la descente du LR jusqu'√† `1e-6` √† la fin de l'entra√Ænement pour garantir la phase de finition ("Fine-tuning").
3.  **R√©gularisation :**
    * Augmentation du **Weight Decay** de `1e-4` √† `1e-3` pour compenser l'augmentation de la taille du mod√®le et √©viter l'overfitting.
    * Ajout de **ColorJitter** (tr√®s l√©ger) dans l'augmentation de donn√©es pour robustifier le mod√®le face aux variations colorim√©triques.